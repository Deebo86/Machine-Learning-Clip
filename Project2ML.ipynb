{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L_TERSdCK7I"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from PIL import Image\n",
        "import clip\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PzPdS27o7oOs",
        "outputId": "337a41ae-a586-4378-f00c-7d05d1e8acb1"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('cat_dog1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePu26VYm71p1"
      },
      "outputs": [],
      "source": [
        "#a-Spliting the dataset into training and testing sets with 20% for test dataset\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.sort_values(train_data.columns[2], axis = 0, inplace=True) #sort by label\n",
        "train_cats = train_data[train_data['labels'] == 0]\n",
        "train_dogs = train_data[train_data['labels'] == 1]\n",
        "#split the training data into cats and dogs so that I can control later on and more easily set the label for calculation of the mean\n",
        "#(I would be able to more easily calculate the mean as I will know for certainty the label of each encoded image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vgj6n4DuVZ1i",
        "outputId": "9b44a9aa-56ed-42d0-b149-1d119e75e369"
      },
      "outputs": [],
      "source": [
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model, transform = clip.load(\"ViT-B/32\", device) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhe841EWchL7"
      },
      "outputs": [],
      "source": [
        "preproc_img_train_cat = []\n",
        "preproc_img_train_dog = []\n",
        "preproc_img_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "dS7GtDfhdKMc",
        "outputId": "65693909-bc93-4689-b863-a86550ff03ee"
      },
      "outputs": [],
      "source": [
        "#Preprocessing of the images to make the encoding process easier and more accurate \n",
        "for file in train_cats['image']:\n",
        "  preproc_img_train_cat.append(transform(Image.open('cat_dog1/' + file )).unsqueeze(0).to(device))\n",
        "for file in train_dogs['image']:\n",
        "  preproc_img_train_dog.append(transform(Image.open('cat_dog1/' + file )).unsqueeze(0).to(device))\n",
        "\n",
        "for file in test_data['image']:\n",
        "  preproc_img_test.append(transform(Image.open('cat_dog1/' + file)).unsqueeze(0).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Re1I9nu0eveV",
        "outputId": "9db5712d-0344-4634-f089-363e30f94791"
      },
      "outputs": [],
      "source": [
        "#encode the inputs\n",
        "image_embeddings_train_dog = []\n",
        "image_embeddings_train_cat = []\n",
        "image_embeddings_test = []\n",
        "torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "  for image in preproc_img_train_cat:\n",
        "    image_embeddings_train_cat.append((model.encode_image(image)).cpu().detach().numpy())\n",
        "  \n",
        "  for image in preproc_img_train_dog:\n",
        "    image_embeddings_train_dog.append((model.encode_image(image)).cpu().detach().numpy())\n",
        "\n",
        "  for image in preproc_img_test:\n",
        "    image_embeddings_test.append((model.encode_image(image)).cpu().detach().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqOKtluBWgVy"
      },
      "outputs": [],
      "source": [
        "#Feature matrices\n",
        "image_embeddings_train_cat = np.vstack(image_embeddings_train_cat)\n",
        "image_embeddings_train_dog = np.vstack(image_embeddings_train_dog)\n",
        "image_embeddings_test = np.vstack(image_embeddings_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoIw8t0KUclK"
      },
      "outputs": [],
      "source": [
        "#Fisher's Algorithm\n",
        "c = 0.1\n",
        "means = []\n",
        "means.append(image_embeddings_train_cat.mean(axis = 0))\n",
        "means.append(image_embeddings_train_dog.mean(axis = 0))\n",
        "S =  np.cov(image_embeddings_train_cat, rowvar=False) + np.cov(image_embeddings_train_dog, rowvar=False)\n",
        "S_inv = np.linalg.inv(S)\n",
        "mean_dif = means[1] - means[0]\n",
        "w = c*np.dot(S_inv, mean_dif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DanOCOOSNzXY"
      },
      "outputs": [],
      "source": [
        "#predictions with c = 0.1\n",
        "predict_c1 = np.dot(image_embeddings_test, w.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmEwLr23OJA1"
      },
      "outputs": [],
      "source": [
        "#Scores\n",
        "#Convert prediction scores into binary class predictions using a 0.0 threshold\n",
        "predicted_labels_c1 = (predict_c1 > 0.0).astype(int)\n",
        "\n",
        "#Calculations of metrics\n",
        "accuracy = accuracy_score(test_data['labels'], predicted_labels_c1)\n",
        "precision = precision_score(test_data['labels'], predicted_labels_c1)\n",
        "recall = recall_score(test_data['labels'], predicted_labels_c1)\n",
        "f1 = f1_score(test_data['labels'], predicted_labels_c1)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_c1 = confusion_matrix(test_data['labels'], predicted_labels_c1)\n",
        "sns.heatmap(confusion_matrix_c1, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dw47Wg4uHJH"
      },
      "outputs": [],
      "source": [
        "#Bonus\n",
        "\n",
        "C = [0.2, 0.3, 0.4, 0.5, 0.7]\n",
        "W = []\n",
        "for c in C:\n",
        "  W.append(c*np.dot(S_inv, mean_dif))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uldlZXd88iYr"
      },
      "outputs": [],
      "source": [
        "predict_C = []\n",
        "for w in W:\n",
        "  predict_C.append(np.dot(image_embeddings_test, w.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqa-wwvkOKsV"
      },
      "outputs": [],
      "source": [
        "#Scores\n",
        "#C = 0.2\n",
        "predicted_labels_C1 = (predict_C[0] > 0.0).astype(int)\n",
        "    \n",
        "#Calculations of metrics\n",
        "accuracy_C1 = accuracy_score(test_data['labels'], predicted_labels_C1)\n",
        "precision_C1 = precision_score(test_data['labels'], predicted_labels_C1)\n",
        "recall_C1 = recall_score(test_data['labels'], predicted_labels_C1)\n",
        "f1_C1 = f1_score(test_data['labels'], predicted_labels_C1)\n",
        "    \n",
        "print('Metrics for c= 0.2:')\n",
        "print('Accuracy:', accuracy_C1)\n",
        "print('Precision:', precision_C1)\n",
        "print('Recall:', recall_C1)\n",
        "print('F1 Score:', f1_C1)\n",
        "    \n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_C1 = confusion_matrix(test_data['labels'], predicted_labels_C1)\n",
        "sns.heatmap(confusion_matrix_C1, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix for c= 0.2')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "#C = 0.3\n",
        "predicted_labels_C2 = (predict_C[1] > 0.0).astype(int)\n",
        "\n",
        "#Calculations of metrics\n",
        "accuracy_C2 = accuracy_score(test_data['labels'], predicted_labels_C2)\n",
        "precision_C2 = precision_score(test_data['labels'], predicted_labels_C2)\n",
        "recall_C2 = recall_score(test_data['labels'], predicted_labels_C2)\n",
        "f1_C2 = f1_score(test_data['labels'], predicted_labels_C2)\n",
        "    \n",
        "print('Metrics for c= 0.3:')\n",
        "print('Accuracy:', accuracy_C2)\n",
        "print('Precision:', precision_C2)\n",
        "print('Recall:', recall_C2)\n",
        "print('F1 Score:', f1_C2)\n",
        "\n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_C2 = confusion_matrix(test_data['labels'], predicted_labels_C2)\n",
        "sns.heatmap(confusion_matrix_C2, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix for c= 0.3')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "#C = 0.4\n",
        "predicted_labels_C3 = (predict_C[2] > 0.0).astype(int)\n",
        "    \n",
        "#Calculations of metrics\n",
        "accuracy_C3 = accuracy_score(test_data['labels'], predicted_labels_C3)\n",
        "precision_C3 = precision_score(test_data['labels'], predicted_labels_C3)\n",
        "recall_C3 = recall_score(test_data['labels'], predicted_labels_C3)\n",
        "f1_C3 = f1_score(test_data['labels'], predicted_labels_C3)\n",
        "    \n",
        "print('Metrics for c= 0.4:')\n",
        "print('Accuracy:', accuracy_C3)\n",
        "print('Precision:', precision_C3)\n",
        "print('Recall:', recall_C3)\n",
        "print('F1 Score:', f1_C3)\n",
        "    \n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_C3 = confusion_matrix(test_data['labels'], predicted_labels_C3)\n",
        "sns.heatmap(confusion_matrix_C3, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix for c= 0.4')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "#C = 0.5\n",
        "predicted_labels_C4 = (predict_C[3] > 0.0).astype(int)\n",
        "    \n",
        "#Calculations of metrics\n",
        "accuracy_C4 = accuracy_score(test_data['labels'], predicted_labels_C4)\n",
        "precision_C4 = precision_score(test_data['labels'], predicted_labels_C4)\n",
        "recall_C4 = recall_score(test_data['labels'], predicted_labels_C4)\n",
        "f1_C4 = f1_score(test_data['labels'], predicted_labels_C4)\n",
        "    \n",
        "print('Metrics for c= 0.5:')\n",
        "print('Accuracy:', accuracy_C4)\n",
        "print('Precision:', precision_C4)\n",
        "print('Recall:', recall_C4)\n",
        "print('F1 Score:', f1_C4)\n",
        "    \n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_C4 = confusion_matrix(test_data['labels'], predicted_labels_C4)\n",
        "sns.heatmap(confusion_matrix_C4, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix for c= 0.5')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "#C = 0.7\n",
        "predicted_labels_C5 = (predict_C[4] > 0.0).astype(int)\n",
        "    \n",
        "#Calculations of metrics\n",
        "accuracy_C5 = accuracy_score(test_data['labels'], predicted_labels_C5)\n",
        "precision_C5 = precision_score(test_data['labels'], predicted_labels_C5)\n",
        "recall_C5 = recall_score(test_data['labels'], predicted_labels_C5)\n",
        "f1_C5 = f1_score(test_data['labels'], predicted_labels_C5)\n",
        "    \n",
        "print('Metrics for c= 0.7:')\n",
        "print('Accuracy:', accuracy_C5)\n",
        "print('Precision:', precision_C5)\n",
        "print('Recall:', recall_C5)\n",
        "print('F1 Score:', f1_C5)\n",
        "    \n",
        "#Generating Confusion Matrix\n",
        "confusion_matrix_C5 = confusion_matrix(test_data['labels'], predicted_labels_C5)\n",
        "sns.heatmap(confusion_matrix_C5, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix for c= 0.7')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
